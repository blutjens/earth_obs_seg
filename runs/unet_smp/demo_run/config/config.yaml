# This config.yaml is to demo the use of this template repository

# Here's where you define experiment-specific hyperparameters.
# You can also create lists and group parameters together into nested sub-parts.
# In Python, this is all read as a dict.

# environment/computational parameters
seed: 42
# device: cpu
num_workers: 4
use_deterministic_algorithms: True # set torch layers to deterministic or raises error
dtype: 'float32' # datatype for in-, output and model weights

model_key: 'unet_smp'           # unique ID for chosen model that is used across filenames, logs, etc.
encoder_name: 'resnet34'        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7
encoder_weights: 'imagenet'     # use `imagenet` pre-trained weights for encoder initialization
in_channels: 3                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)
out_channels: 1                 # number of model output channels, e.g., 1 for binary classification

# Replace <demo_run> with the name of your data split
experiment_key: 'demo_run'
path_data: './data/hrmelt_sample/' # Replace this with your data path
path_checkpoints: './runs/unet_smp/demo_run/checkpoints' # Store model checkpoints

# These are the full paths to every channel the dataloader should have access to
path_mar_wa1: './data/hrmelt_sample/MAR/MARv3.12/WA1/'
path_pmw: './data/hrmelt_sample/PMW/'
path_melt: './data/hrmelt_sample/SAR/'
# Paths to static variables
path_landmask: './data/hrmelt_sample/landmask/landMask_100m.tif'
path_dem: './data/hrmelt_sample/DEM/dem_100m.tif'

# training hyperparameters
epochs: 100
batch_size: 2
optimizer: 'adamW' # Optimization algorithm
learning_rate: 0.0001
weight_decay: 0.
loss_function: 'dice' # E.g., 'dice' for binary image segmentation

# Step learning rate scheduler 
lr_scheduler: 'ReduceLROnPlateau'
lr_patience: 100 # Number of steps on validation loss platenau until lr scheduler reduces lr

# CosineAnnealingWarmRestarts lerning rate scheduler
# lr_scheduler: 'CosineAnnealingWarmRestarts'
T_0: 35 # Number of epochs until first restart; 
T_mult: 2 # A factor increases Ti after a restart. 
# To understand this see: https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863
# E.g., First restart after T_0 epochs and then 2nd restart after T_0*T_mult epochs
# With T_0 = 35 and T_mult = 2, the training can be nicely stopped at 525 epochs: (1+ 2+ 4+ 8) *35 = 525
eta_min: 0. # Minimum learning rate. Default: 0.
